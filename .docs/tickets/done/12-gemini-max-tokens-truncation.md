# Ticket 12: Gemini Audio Transcription MAX_TOKENS Truncation

## Priority: HIGH

## Status: Implemented - Testing Required

## Summary

Gemini 2.5 Flash audio transcription responses are being truncated due to hitting the MAX_TOKENS limit, resulting in incomplete transcriptions. The response ends mid-sentence with `"finishReason": "MAX_TOKENS"` instead of `"STOP"`, indicating the model had more to generate but was cut off by the token limit.

## Problem Description

When transcribing audio files, the Gemini API response is being truncated before completion:

- **Model**: `gemini-2.5-flash`
- **Audio Input**: 19,203 audio tokens + 369 text tokens
- **Output**: 3,090 tokens (truncated at MAX_TOKENS limit)
- **Thoughts Tokens**: 8,896 tokens (internal reasoning)
- **Finish Reason**: `MAX_TOKENS` (model didn't complete naturally)
- **Response ID**: `Q3iAaY-XNYW3nsEPzp_X-Q8`

The transcription JSON itself is cut off mid-field:

```json
{
  "timeSeconds": 906.0,
  "text": "Daar zijn drie tempels die op de kaart zijn."
},
{"
```

## Impact

1. **Data Loss**: Users receive incomplete transcriptions, losing potentially critical session content
2. **User Experience**: Transcriptions end abruptly without warning
3. **No Recovery**: Truncated responses provide no indication of what was lost
4. **Storage Corruption**: Incomplete/malformed JSON may be stored in database
5. **Silent Failure**: Users may not realize their transcription is incomplete

## Root Cause Analysis

The `finishReason: "MAX_TOKENS"` indicates the model hit the output token limit configured in the API request. Potential causes:

1. **No max_tokens specified**: Using API default, which may be too low
2. **Insufficient max_tokens**: Set too low for the input size
3. **Model verbosity**: Gemini 2.5 may generate more verbose output than expected
4. **JSON overhead**: Large JSON structure with timestamps increases token usage
5. **Thoughts tokens**: The 8,896 thought tokens may count toward the limit

## Token Budget Analysis

For this specific case:

- **Input**: 19,572 tokens (19,203 audio + 369 text)
- **Output**: 3,090 tokens (actual returned)
- **Thoughts**: 8,896 tokens
- **Total**: 31,558 tokens

The thoughts tokens are substantial and may be consuming budget that should go to output.

### Recommended Token Budget Formula

```typescript
// For audio transcription
const estimatedOutputTokens = Math.ceil(audioTokens * 0.3); // 30% of input
const maxOutputTokens = Math.max(
  estimatedOutputTokens,
  4000 // Minimum reasonable output
);

// Example: 19,203 audio tokens â†’ ~5,761 output tokens needed
// Current limit appears to be ~3,090 â†’ insufficient
```

## Proposed Solutions

### Immediate (Hotfix)

1. **Fix Temperature Settings** âš ï¸ HIGHEST PRIORITY

   ```typescript
   // In audio-transcription.service.ts, line ~229
   config: {
     responseMimeType: 'application/json',
     responseSchema: this.TRANSCRIPTION_SCHEMA,
     maxOutputTokens: this.MAX_TRANSCRIPTION_OUTPUT_TOKENS,
     temperature: 0.9,        // CHANGE FROM 0.2 â†’ 0.9
     topP: 0.95,              // CHANGE FROM 0.9 â†’ 0.95
     topK: 40,                // CHANGE FROM 20 â†’ 40
     stopSequences: [         // ADD STOP SEQUENCES
       '\n\n\n\n',            // Multiple blank lines
       'Nee, nee, nee, nee'   // Detected repetition pattern
     ]
   }
   ```

   **Why this matters:**

   - Temperature 0.2 is appropriate for classification/multiple-choice tasks
   - Temperature 0.9 is better for transcription - allows natural language variety
   - Higher topP and topK give model more token options, reducing repetition likelihood
   - Stop sequences provide emergency brake if repetition starts

2. **Improve Prompt with Anti-Repetition Instructions**

   Update `audio-transcription.prompt.ts`:

   - Add explicit anti-repetition rules
   - Provide output format examples
   - Include guidance for unclear audio

   ```typescript
   ANTI-REPETITION RULES:
   - NEVER repeat the same word more than 3 times in a row
   - If you find yourself generating repetitive content, STOP immediately
   - Each segment should contain unique, meaningful dialogue
   - If audio is unclear, use [inaudible] rather than guessing or repeating

   OUTPUT FORMAT EXAMPLE:
   {
     "segments": [
       { "timeSeconds": 0, "text": "The party enters the dungeon.", "speaker": "DM" },
       { "timeSeconds": 15, "text": "I check for traps.", "speaker": "Player1" }
     ]
   }
   ```

3. **Add Repetition Detection in Response Validation**

   ```typescript
   private detectRepetition(text: string): boolean {
     // Check for word-level repetition (e.g., "nee, nee, nee...")
     const words = text.toLowerCase().split(/[\s,]+/).filter(w => w.length > 2);

     if (words.length < 5) return false;

     // Check if same word appears 5+ times in a row
     for (let i = 0; i < words.length - 4; i++) {
       const word = words[i];
       if (words[i+1] === word &&
           words[i+2] === word &&
           words[i+3] === word &&
           words[i+4] === word) {
         console.error(`Repetition detected: "${word}" repeated 5+ times`);
         return true;
       }
     }

     // Check for sentence-level repetition
     const segments = text.split(/[.!?]+/).filter(s => s.trim().length > 20);
     const uniqueSegments = new Set(segments.map(s => s.trim().toLowerCase()));
     const repetitionRatio = 1 - (uniqueSegments.size / segments.length);

     if (repetitionRatio > 0.5) {
       console.error(`High repetition ratio: ${(repetitionRatio * 100).toFixed(1)}%`);
       return true;
     }

     return false;
   }
   ```

4. **Increase max_tokens Parameter**

   - Set explicit max_tokens based on input size
   - Use formula: `max_tokens = Math.ceil(audioTokens * 0.4) + 1000`
   - Add safety margin for JSON overhead and timestamps

5. **Add Truncation Detection**

   - Check `finishReason` field in response
   - If `MAX_TOKENS`, log warning and potentially retry with higher limit
   - Alert user that transcription may be incomplete

6. **Validate JSON Completeness**

   - Attempt to parse response JSON
   - Check if `segments` array is properly closed
   - Detect incomplete/malformed JSON structures

7. **Monitor Thought Tokens**

   ```typescript
   // Log thought token usage to detect model confusion
   if (response.usageMetadata.thoughtsTokenCount > response.usageMetadata.promptTokenCount) {
     console.warn(
       `Excessive thought tokens: ${response.usageMetadata.thoughtsTokenCount} ` +
         `(${Math.round(
           response.usageMetadata.thoughtsTokenCount / response.usageMetadata.promptTokenCount
         )}x input)`
     );
   }
   ```

### Short-term

1. **Dynamic Token Budgeting**

   ```typescript
   interface TokenBudget {
     maxOutputTokens: number;
     reserveForThoughts: number;
     totalRequestLimit: number;
   }

   private calculateTokenBudget(audioTokens: number): TokenBudget {
     // Gemini 2.5 Flash has ~1M token context window
     const totalLimit = 1_000_000;

     // Reserve for thoughts (Gemini uses these internally)
     const thoughtsReserve = Math.ceil(audioTokens * 0.5);

     // Calculate max output (be generous)
     const maxOutput = Math.min(
       Math.ceil(audioTokens * 0.5), // 50% of input
       100_000 // Cap at 100k output tokens
     );

     return {
       maxOutputTokens: maxOutput,
       reserveForThoughts: thoughtsReserve,
       totalRequestLimit: totalLimit
     };
   }
   ```

2. **Response Streaming**

   - Use streaming API if available
   - Monitor token usage in real-time
   - Implement graceful cutoff if approaching limit

3. **Chunking Strategy**
   - For long audio (>30 minutes), split into chunks
   - Process each chunk separately with adequate token budget
   - Merge results (see Ticket 10 for chunking implementation)

### Long-term

1. **Multi-Pass Transcription**

   - First pass: Quick transcription with timestamps
   - Second pass: Refinement and speaker identification
   - Allows better token budget allocation

2. **Compression Strategies**

   - Use more compact timestamp format
   - Reduce JSON overhead
   - Request concise output format

3. **Alternative Models**
   - Test Gemini Pro or other models with different token economics
   - Compare output length vs. quality trade-offs

## Implementation Plan

### Phase 1: Emergency Fix (Today)

```typescript
// Add to audio-transcription.service.ts

interface GeminiRequest {
  maxOutputTokens?: number;
  // ... other params
}

interface GeminiResponse {
  candidates: Array<{
    content: { parts: Array<{ text: string }> };
    finishReason: 'STOP' | 'MAX_TOKENS' | 'SAFETY' | 'RECITATION' | 'OTHER';
  }>;
  usageMetadata: {
    promptTokenCount: number;
    candidatesTokenCount: number;
    totalTokenCount: number;
    thoughtsTokenCount?: number;
  };
}

private calculateMaxTokens(audioTokens: number): number {
  // Conservative formula: 50% of input + 1000 buffer
  const baseEstimate = Math.ceil(audioTokens * 0.5);
  const buffer = 1000;
  const minimum = 4000;

  return Math.max(baseEstimate + buffer, minimum);
}

private validateTranscriptionResponse(response: GeminiResponse): {
  valid: boolean;
  error?: string;
  warning?: string;
} {
  const candidate = response.candidates[0];

  // Check finish reason
  if (candidate.finishReason === 'MAX_TOKENS') {
    return {
      valid: false,
      error: 'Transcription was truncated due to token limit',
      warning: `Used ${response.usageMetadata.candidatesTokenCount} tokens, may be incomplete`
    };
  }

  // Check JSON validity
  try {
    const text = candidate.content.parts[0].text;
    const parsed = JSON.parse(text);

    if (!parsed.segments || !Array.isArray(parsed.segments)) {
      return {
        valid: false,
        error: 'Invalid transcription format: missing segments array'
      };
    }

    // Check if segments array is properly terminated
    if (!text.trim().endsWith('}')) {
      return {
        valid: false,
        error: 'Malformed JSON: response appears truncated'
      };
    }

  } catch (err) {
    return {
      valid: false,
      error: `JSON parse error: ${err.message}`
    };
  }

  return { valid: true };
}

async transcribeAudioWithRetry(
  audioFile: File,
  maxRetries: number = 2
): Promise<TranscriptionResult> {
  const audioTokens = await this.estimateAudioTokens(audioFile);
  let maxTokens = this.calculateMaxTokens(audioTokens);

  for (let attempt = 0; attempt < maxRetries; attempt++) {
    const response = await this.callGeminiWithAudio(audioFile, {
      maxOutputTokens: maxTokens,
      // ... other params
    });

    const validation = this.validateTranscriptionResponse(response);

    if (validation.valid) {
      return this.parseTranscriptionResponse(response);
    }

    // If truncated, increase token limit and retry
    if (response.candidates[0].finishReason === 'MAX_TOKENS') {
      console.warn(
        `Attempt ${attempt + 1}: Transcription truncated at ${response.usageMetadata.candidatesTokenCount} tokens`
      );

      // Increase limit by 50% for next attempt
      maxTokens = Math.ceil(maxTokens * 1.5);

      console.log(`Retrying with maxTokens: ${maxTokens}`);
      continue;
    }

    // Other error, don't retry
    throw new Error(validation.error || 'Transcription failed');
  }

  throw new Error('Failed to complete transcription after retries');
}
```

### Phase 2: Monitoring & Alerting (This Week)

```typescript
// Track token usage patterns
interface TranscriptionMetrics {
  audioTokens: number;
  outputTokens: number;
  thoughtsTokens: number;
  finishReason: string;
  duration: number;
  timestamp: Date;
}

private async logTranscriptionMetrics(metrics: TranscriptionMetrics) {
  // Log to analytics service
  console.log('[METRICS] Transcription completed:', {
    audioLength: metrics.audioTokens,
    outputLength: metrics.outputTokens,
    thoughts: metrics.thoughtsTokens,
    ratio: (metrics.outputTokens / metrics.audioTokens).toFixed(2),
    finishReason: metrics.finishReason,
    durationMs: metrics.duration
  });

  // Alert if truncated
  if (metrics.finishReason === 'MAX_TOKENS') {
    console.error('[ALERT] Transcription truncated!', metrics);
    // Send to monitoring service (Sentry, CloudWatch, etc.)
  }
}
```

### Phase 3: Chunking Implementation (Next Sprint)

Implement audio chunking strategy as detailed in Ticket 10, which will naturally solve token limit issues by processing smaller segments.

## Testing

### Test Cases

1. **Token Budget Tests**

   - [ ] Test with 10-minute audio (~2.5k tokens)
   - [ ] Test with 30-minute audio (~7.5k tokens)
   - [ ] Test with 60-minute audio (~15k tokens)
   - [ ] Test with 2-hour audio (~30k tokens)
   - [ ] Verify max_tokens is set appropriately for each

2. **Truncation Detection Tests**

   - [ ] Simulate MAX_TOKENS response
   - [ ] Verify validation catches truncation
   - [ ] Confirm retry mechanism works
   - [ ] Test retry with increased token limit

3. **JSON Validation Tests**

   - [ ] Valid complete JSON passes
   - [ ] Truncated JSON is detected
   - [ ] Malformed segments array is caught
   - [ ] Empty response handled gracefully

4. **Edge Cases**
   - [ ] Very short audio (<1 minute)
   - [ ] Exactly 30 minutes (boundary case)
   - [ ] Very long audio (>3 hours)
   - [ ] Silent audio (minimal output needed)
   - [ ] Multiple retries all fail

## Acceptance Criteria

- [ ] All transcription requests set explicit max_tokens parameter
- [ ] Token budget calculated based on input size with safety margin
- [ ] finishReason is checked and MAX_TOKENS is handled appropriately
- [ ] Retry mechanism implemented with increasing token limits
- [ ] JSON validation catches truncated/malformed responses
- [ ] Users notified if transcription is incomplete
- [ ] Metrics logged for token usage patterns
- [ ] No transcriptions lost due to silent truncation

## Related Files

- `src/app/audio/audio-transcription.service.ts` - Transcription logic
- `src/app/audio/audio-session.models.ts` - Type definitions
- `src/environments/environment.ts` - API configuration

## Related Tickets

- **Ticket 10**: AI Summary Repetition Bug - Related Gemini API issues
- **Ticket 11**: Gemini Summary Repetition Fix - Chunking strategy applicable here

## Example Response Data

<details>
<summary>Full truncated response (click to expand)</summary>

```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "{\n  \"segments\": [\n    {\n      \"timeSeconds\": 600.0,\n      \"text\": \"jij wilde deze onderzoeken.\"\n    },\n    {\n      \"timeSeconds\": 602.0,\n      \"text\": \"Misschien helpt dat. Dan blijft hij dichtbij staan.\"\n    },\n    ...\n    {\n      \"timeSeconds\": 906.0,\n      \"text\": \"Daar zijn drie tempels die op de kaart zijn.\"\n    },\n    {\"  // <-- TRUNCATED HERE
          }
        ],
        "role": "model"
      },
      "finishReason": "MAX_TOKENS",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 19572,
    "candidatesTokenCount": 3090,
    "totalTokenCount": 31558,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 369
      },
      {
        "modality": "AUDIO",
        "tokenCount": 19203
      }
    ],
    "thoughtsTokenCount": 8896
  },
  "modelVersion": "gemini-2.5-flash",
  "responseId": "Q3iAaY-XNYW3nsEPzp_X-Q8"
}
```

</details>

## UPDATE: Critical Repetition + Thought Token Issue

**Second Response Analysis** (Response ID: `GXuAabCxDvfnnsEPw5DjoQ8`):

Even with increased token budget, a new critical issue emerged:

```json
{
  "text": "Nee, nee, nee, nee, nee, nee..." // Repeated ~2500 times
  "finishReason": "MAX_TOKENS",
  "candidatesTokenCount": 2549,
  "thoughtsTokenCount": 61437  // âš ï¸ MASSIVE - 3x the audio input!
}
```

### Root Cause Identified

**Current Configuration** (Line 225-232 in `audio-transcription.service.ts`):

```typescript
config: {
  temperature: 0.2,        // âš ï¸ TOO LOW - encourages repetition
  topP: 0.9,
  topK: 20,
  maxOutputTokens: 64000
}
```

**Problems:**

1. **Temperature 0.2 is too low for transcription**: Low temperature makes the model deterministic and prone to getting stuck in repetitive loops. For creative/generative tasks like transcription, higher temperature (0.7-1.0) provides better variety.

2. **Massive thought tokens (61,437)**: The model is burning 3x the audio tokens just "thinking," suggesting it's confused, struggling, or stuck in an internal reasoning loop.

3. **Prompt lacks anti-repetition guidance**: Current prompt doesn't explicitly tell the model to avoid repetition or provide output examples.

4. **No stop sequences configured**: Nothing prevents runaway generation of the same word/phrase.

5. **Audio quality issues**: The repetitive "Nee, nee, nee..." might indicate:
   - Audio contains actual repetitive content that confused the model
   - Poor audio quality causing transcription uncertainty
   - Background noise being mistranscribed as repeated words

## Notes

- The 8,896 thoughts tokens in first response were significant - nearly 3x the output tokens
- The 61,437 thoughts tokens in second response are catastrophic - 3x the audio input
- Thoughts tokens suggest model is stuck in internal reasoning loops
- This may be a Gemini 2.5 Flash feature where the model "thinks" internally
- Consider whether thoughts tokens count against max_tokens limit
- Dutch language transcription appears accurate up to truncation point in first response
- Second response shows complete hallucination/repetition failure
- Timestamps are well-formatted (simple float seconds format)

## Implementation Status

### âœ… Completed (2026-02-02)

**1. Configuration Changes** (`audio-transcription.service.ts` line ~229)

- âœ… Temperature: 0.2 â†’ 0.9 (prevents repetition loops)
- âœ… topP: 0.9 â†’ 0.95 (more token variety)
- âœ… topK: 20 â†’ 40 (wider selection pool)
- âœ… Added stopSequences: `['\n\n\n\n']` (emergency brake)

**2. Response Validation** (`audio-transcription.service.ts`)

- âœ… Added `validateTranscriptionResponse()` method
  - Checks for MAX_TOKENS finish reason
  - Monitors excessive thought token usage (>2x input)
  - Validates JSON structure completeness
  - Logs token usage metrics
- âœ… Added `detectRepetition()` method
  - Detects 5+ consecutive identical words
  - Calculates repetition ratio across segments
  - Logs detailed repetition warnings

**3. Prompt Engineering** (`audio-transcription.prompt.ts`)

- âœ… Added ANTI-REPETITION RULES section
  - Explicit instructions to never repeat >3 times
  - Guidance to use [inaudible]/[unclear] for uncertain audio
  - Emphasis on stopping if generating repetitive content
- âœ… Added QUALITY GUIDELINES section
  - Prefer brevity over invention
  - Skip unclear segments rather than guessing
- âœ… Added OUTPUT FORMAT EXAMPLE
  - Shows proper JSON structure
  - Demonstrates good segment formatting
  - Includes speaker labels and timestamps

**4. Monitoring & Logging**

- âœ… Token usage metrics logged for each transcription
- âœ… Thought token ratio tracked (thoughts/input)
- âœ… Warnings for excessive thought tokens
- âœ… Error messages for truncation and repetition

### ðŸ“‹ Testing Required

- [ ] Test with 10-minute clean audio (baseline)
- [ ] Test with 30-minute audio (token budget)
- [ ] Test with 60-minute audio (long duration)
- [ ] Test with poor quality/noisy audio (edge case)
- [ ] Test with silent/empty audio (error handling)
- [ ] Verify no catastrophic repetition occurs
- [ ] Verify thought tokens < 1.5x input (95% of cases)
- [ ] Verify finishReason = "STOP" (not MAX_TOKENS)
- [ ] Verify complete, valid JSON responses

### Expected Improvements

**Token Usage:**

- Before: 83,558 tokens (61,437 thoughts + 2,549 output) = **FAILED**
- After: ~35,000 tokens (~10,000 thoughts + ~6,000 output) = **SUCCESS**
- Savings: **~58% token reduction**

**Success Rate:**

- Before: 0% (repetitive nonsense or truncated)
- After: >95% (complete, accurate transcriptions)

## References

- [Gemini API Token Limits](https://ai.google.dev/gemini-api/docs/tokens)
- [Gemini API Response Format](https://ai.google.dev/api/rest/v1/GenerateContentResponse)
- [Managing Token Budgets](https://ai.google.dev/gemini-api/docs/models/gemini)
- [Temperature Parameter Best Practices](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters)
